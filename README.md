# Tutorial: Web scraping con Scrapling

Este repositorio es un **tutorial paso a paso** para aprender a extraer datos de la web con Python usando la librer√≠a [Scrapling](https://github.com/D4Vinci/Scrapling). Al final sabr√°s hacer peticiones HTTP, usar selectores CSS para sacar texto y enlaces, y montar un scraper que guarde resultados en JSON.

**Qu√© vamos a hacer:**

1. Preparar el entorno e instalar Scrapling.
2. Hacer tu primera extracci√≥n: traer citas de una p√°gina de ejemplo.
3. Entender selectores CSS y el parser de Scrapling.
4. Construir un scraper completo para b√∫squedas en arXiv y guardar los datos en un archivo.

---

## Qu√© es Scrapling

Scrapling es un framework de web scraping en Python. Permite:

- **Traer p√°ginas:** con `Fetcher` (HTTP simple), o con navegador para sitios con JavaScript o anti-bot.
- **Extraer datos:** con selectores CSS o XPath, al estilo Scrapy/Parsel (`.css()`, `.get()`, `.getall()`, `::text`, etc.).
- **Escalar:** crawls con muchas URLs, concurrencia y spiders (lo ver√°s en la documentaci√≥n cuando quieras ir m√°s all√°).

En este tutorial usamos solo **Fetcher** (peticiones HTTP) y el **parser** (Selector) para quedarnos en lo esencial.

Documentaci√≥n completa: [scrapling.readthedocs.io](https://scrapling.readthedocs.io).

---

## 1. Preparaci√≥n

### Requisitos

- **Python 3.10 o superior**
- Dependencias: `scrapling` y `requests` (para el ejemplo de arXiv)

### Instalaci√≥n

Abre una terminal en la carpeta del proyecto y ejecuta:

```bash
# Crear entorno virtual (recomendado)
python -m venv venv

# Activar el entorno
# En Linux/macOS:
source venv/bin/activate
# En Windows:
# venv\Scripts\activate

# Instalar Scrapling y requests
pip install scrapling requests
```

Comprueba que todo va bien:

```bash
python -c "from scrapling.fetchers import Fetcher; print('OK')"
```

Si ves `OK`, est√°s listo para el siguiente paso.

---

## 2. Tu primera extracci√≥n

Vamos a extraer las **citas** de la p√°gina de prueba [Quotes to Scrape](https://quotes.toscrape.com). Es un sitio pensado para practicar scraping.

### Paso 2.1 ‚Äî Traer la p√°gina

Scrapling puede hacer la petici√≥n HTTP y devolver un objeto con el que luego extraer datos. Se usa la clase `Fetcher` y el m√©todo `.get()`:

```python
from scrapling.fetchers import Fetcher

page = Fetcher.get("https://quotes.toscrape.com")
```

`page` no es solo el HTML en texto: es un **Selector** sobre ese HTML. Con √©l podemos buscar elementos dentro de la p√°gina.

### Paso 2.2 ‚Äî Elegir qu√© extraer con CSS

En Quotes to Scrape, cada cita est√° dentro de un `<div class="quote">`, y el texto de la cita est√° en un `<span class="text">`. Para quedarnos solo con el **texto** (sin etiquetas HTML) usamos el pseudo-elemento `::text`:

```python
quotes = page.css(".quote .text::text").getall()
```

- `.quote` ‚Üí elementos con clase `quote`
- `.text` ‚Üí dentro de ellos, elementos con clase `text`
- `::text` ‚Üí solo el texto interior
- `.getall()` ‚Üí devuelve una **lista** con todos los resultados (si usas `.get()` obtienes solo el primero)

### Paso 2.3 ‚Äî Mostrar los resultados

```python
print("üìú Citas encontradas:\n")
for q in quotes[:5]:   # las 5 primeras
    print("-", q)
```

### Ejecutar el ejemplo

En el proyecto ya tienes esto en `main.py`. Ejecuta:

```bash
python main.py
```

Deber√≠as ver algo como:

```
üìú Quotes encontradas:

- "The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking."
- "It is our choices, Harry, that show what we truly are, far more than our abilities."
...
```

**Resumen:** `Fetcher.get(url)` trae la p√°gina; `page.css("selector::text").getall()` extrae una lista de textos. Con eso ya puedes scrapear muchas p√°ginas est√°ticas.

---

## 3. Entender selectores y el parser

### Selectores CSS

Con `.css()` puedes usar lo que ya conoces de CSS:

| Selector      | Ejemplo                    | Significado                          |
|---------------|----------------------------|--------------------------------------|
| Clase         | `.quote`                   | elementos con `class="quote"`        |
| Varias clases | `.quote.text`              | elementos que tengan ambas clases    |
| Descendiente  | `.quote .text`             | `.text` dentro de `.quote`           |
| Pseudo texto  | `.text::text`              | solo el texto dentro de `.text`      |
| Atributo      | `a::attr(href)`            | valor del atributo `href` del enlace |

- **`.get()`** ‚Üí el primer resultado o `None`.
- **`.getall()`** ‚Üí lista con todos los resultados.

### Usar solo el parser (sin Fetcher)

Si ya tienes el HTML (por ejemplo descargado con `requests`), puedes usar el parser de Scrapling sin hacer la petici√≥n t√∫ mismo:

```python
from scrapling.parser import Selector

html = "<div class='quote'><span class='text'>Una cita</span></div>"
page = Selector(html)
texto = page.css(".quote .text::text").get()
# texto == "Una cita"
```

En el siguiente apartado haremos exactamente eso: descargamos el HTML con `requests` y extraemos los datos con `Selector`.

---

## 4. Scraper completo: b√∫squedas en arXiv

Vamos a construir un script que busque art√≠culos en [arXiv](https://arxiv.org), extraiga t√≠tulo, autores, fecha, resumen y enlaces, y guarde todo en un JSON.

### Paso 4.1 ‚Äî Descargar el HTML de la b√∫squeda

arXiv permite buscar por URL. Construimos la URL con la query y el n√∫mero de resultados (25, 50, 100 o 200) y descargamos con `requests`:

```python
import requests
from urllib.parse import quote_plus

query = "machine learning"
size = 25
url = (
    "https://arxiv.org/search/?"
    f"query={quote_plus(query)}"
    "&searchtype=all&abstracts=show&order=-announced_date_first"
    f"&size={size}"
)
html = requests.get(url, headers={"User-Agent": "Mozilla/5.0 ..."}, timeout=30).text
```

(En el archivo `ArXiv-scrapling.py` esta l√≥gica est√° en la funci√≥n `fetch_html()`.)

### Paso 4.2 ‚Äî Parsear con Scrapling

Pasamos el HTML al parser y buscamos cada resultado. En la p√°gina de b√∫squeda de arXiv, cada art√≠culo est√° en un `<li class="arxiv-result">`. Dentro hay p√°rrafos y spans con clases concretas para t√≠tulo, autores, fecha y resumen:

```python
from scrapling.parser import Selector

page = Selector(html)
results = page.css("li.arxiv-result")

for r in results:
    title = r.css("p.title.is-5.mathjax::text").get()
    authors = r.css("p.authors a::text").getall()
    # ... m√°s campos
```

Aqu√≠ vemos que **sobre cada elemento** (`r`) podemos volver a usar `.css()` para buscar dentro de √©l. As√≠ sacamos t√≠tulo, autores, abstract, etc., de cada bloque.

### Paso 4.3 ‚Äî Guardar en JSON

Cada art√≠culo lo guardamos como un diccionario con `title`, `authors`, `submitted`, `abstract`, `url`, `pdf_url`. Al final, volcamos la lista a un archivo:

```python
import json

with open("arxiv_results.json", "w", encoding="utf-8") as f:
    json.dump(data, f, ensure_ascii=False, indent=2)
```

### Ejecutar el scraper de arXiv

El proyecto incluye todo esto en `ArXiv-scrapling.py`. Por defecto busca *"retrieval augmented generation"* y guarda 25 resultados. Ejecuta:

```bash
python ArXiv-scrapling.py
```

Ver√°s un resumen en la terminal y se crear√° el archivo `arxiv_results.json`. Puedes cambiar la b√∫squeda o el `size` editando las variables al final del archivo:

```python
if __name__ == "__main__":
    query = "retrieval augmented generation"  # cambia aqu√≠
    data = scrape_arxiv_search(query=query, size=25)
    # ...
```

**Resumen:** combinamos `requests` para descargar y `Selector(html)` de Scrapling para extraer. El flujo es siempre: obtener HTML ‚Üí Selector ‚Üí `.css()` / `.get()` / `.getall()` ‚Üí estructurar datos ‚Üí guardar (JSON, CSV, etc.).

---

## 5. Resumen y siguientes pasos

Has aprendido a:

- Instalar Scrapling y hacer una petici√≥n con `Fetcher.get()`.
- Extraer datos con `.css()` y los pseudo-elementos `::text` y `::attr(href)`.
- Usar `.get()` y `.getall()` y entender la diferencia.
- Usar el parser `Selector(html)` con HTML que ya tengas (por ejemplo con `requests`).
- Montar un scraper que guarde resultados en JSON.

### Ideas para seguir

- **Probar otras p√°ginas:** usa [Quotes to Scrape](https://quotes.toscrape.com) para practicar m√°s selectores (autores, enlaces, paginaci√≥n).
- **Documentaci√≥n de Scrapling:** [scrapling.readthedocs.io](https://scrapling.readthedocs.io) ‚Äî fetchers avanzados (navegador, anti-bot), spiders, proxies, CLI.
- **Respetar el sitio:** revisa `robots.txt` y condiciones de uso; no hagas demasiadas peticiones seguidas.

---

## Estructura del proyecto

```
Scrapling/
‚îú‚îÄ‚îÄ main.py              # Tutorial paso 2: primera extracci√≥n (Quotes)
‚îú‚îÄ‚îÄ ArXiv-scrapling.py   # Tutorial paso 4: scraper arXiv
‚îú‚îÄ‚îÄ arxiv_results.json   # Generado al ejecutar ArXiv-scrapling.py
‚îú‚îÄ‚îÄ venv/                # Entorno virtual (no subir a git)
‚îî‚îÄ‚îÄ README.md            # Este tutorial
```

*Scrapling est√° bajo licencia BSD-3-Clause. Este repo es solo para aprendizaje.*
